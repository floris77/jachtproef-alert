#!/usr/bin/env python3
"""
Validate the updated scraper results and show the improvements
"""

import json
import re
from collections import defaultdict

def validate_updated_scraper_results():
    """Parse and validate the updated scraper results"""
    
    # The results from the updated scraper
    scraper_output = """{"success": true, "tier1_matches": 0, "tier2_matches": 151, "tier2_breakdown": {"veldwedstrijd": 90, "jachthondenproef": 60, "orweja_werktest": 1}, "final_matches": 151, "matches_uploaded": 151, "using_tier1": false, "timestamp": "2025-07-06T19:49:01.713981", "tier1_data": [], "tier2_data": [{"date": "2025-07-24", "organizer": "Chesapeake Bay Retriever Club Nederland [email protected]", "location": "West Brabant", "remarks": "", "registration_text": "Inschrijven", "type": "CAC Apporteerwedstrijd", "source": "tier2", "calendar_type": "Veldwedstrijd"}]}"""
    
    print("üîç UPDATED SCRAPER VALIDATION RESULTS")
    print("=" * 80)
    
    # Parse the JSON response
    try:
        data = json.loads(scraper_output)
    except json.JSONDecodeError as e:
        print(f"‚ùå Failed to parse JSON: {e}")
        return
    
    # Extract key metrics
    tier1_matches = data.get('tier1_matches', 0)
    tier2_matches = data.get('tier2_matches', 0)
    final_matches = data.get('final_matches', 0)
    using_tier1 = data.get('using_tier1', False)
    
    print(f"üìä SCRAPER CONFIGURATION:")
    print(f"   Using Tier 1: {using_tier1}")
    print(f"   Tier 1 matches: {tier1_matches}")
    print(f"   Tier 2 matches: {tier2_matches}")
    print(f"   Final matches: {final_matches}")
    
    # Tier 2 breakdown
    tier2_breakdown = data.get('tier2_breakdown', {})
    print(f"\nüìä TIER 2 BREAKDOWN:")
    print(f"   Veldwedstrijd: {tier2_breakdown.get('veldwedstrijd', 0)}")
    print(f"   Jachthondenproef: {tier2_breakdown.get('jachthondenproef', 0)}")
    print(f"   ORWEJA Werktest: {tier2_breakdown.get('orweja_werktest', 0)}")
    
    # Analyze Tier 2 data quality
    tier2_data = data.get('tier2_data', [])
    if not tier2_data:
        print("‚ùå No Tier 2 data found in response")
        return
    
    print(f"\nüîç TIER 2 DATA QUALITY ANALYSIS:")
    print(f"   Total matches analyzed: {len(tier2_data)}")
    
    # Group by calendar type
    by_calendar = defaultdict(list)
    for match in tier2_data:
        calendar_type = match.get('calendar_type', 'unknown')
        by_calendar[calendar_type].append(match)
    
    # Analyze each calendar
    total_issues = 0
    all_suspicious_matches = []
    
    for calendar_type, calendar_matches in by_calendar.items():
        print(f"\n   {calendar_type.upper()} CALENDAR:")
        
        # Count empty fields
        empty_organizers = sum(1 for m in calendar_matches if not clean_text(m.get('organizer', '')))
        empty_locations = sum(1 for m in calendar_matches if not clean_text(m.get('location', '')))
        empty_types = sum(1 for m in calendar_matches if not clean_text(m.get('type', '')))
        
        total = len(calendar_matches)
        print(f"     Matches: {total}")
        print(f"     Empty organizers: {empty_organizers}/{total} ({empty_organizers/total*100:.1f}%)")
        print(f"     Empty locations: {empty_locations}/{total} ({empty_locations/total*100:.1f}%)")
        print(f"     Empty types: {empty_types}/{total} ({empty_types/total*100:.1f}%)")
        
        # Check for field mapping issues
        types_in_organizer = sum(1 for m in calendar_matches if re.search(r'(MAP|KNJV|veldwedstrijd|TAP|CAC|CACIT)', clean_text(m.get('organizer', ''))))
        orgs_in_type = sum(1 for m in calendar_matches if re.search(r'(Stichting|Vereniging|Club)', clean_text(m.get('type', ''))))
        
        print(f"     Match types in organizer field: {types_in_organizer}")
        print(f"     Organization names in type field: {orgs_in_type}")
        
        # Track suspicious matches
        suspicious_matches = []
        for match in calendar_matches:
            issues = []
            
            organizer = clean_text(match.get('organizer', ''))
            location = clean_text(match.get('location', ''))
            match_type = clean_text(match.get('type', ''))
            
            # Check for field mapping issues
            if re.search(r'(MAP|KNJV|veldwedstrijd|TAP|CAC|CACIT)', organizer):
                issues.append("Match type in organizer field")
            
            if re.search(r'(Stichting|Vereniging|Club)', match_type):
                issues.append("Organization name in type field")
            
            # Check for empty critical fields
            if not organizer and not location:
                issues.append("Empty organizer and location")
            
            if issues:
                suspicious_matches.append({
                    'match': match,
                    'issues': issues,
                    'calendar_type': calendar_type
                })
        
        print(f"     Suspicious matches: {len(suspicious_matches)}")
        all_suspicious_matches.extend(suspicious_matches)
        total_issues += len(suspicious_matches)
        
        # Show sample matches
        print(f"     Sample matches:")
        for i, match in enumerate(calendar_matches[:3]):
            organizer = clean_text(match.get('organizer', ''))[:40]
            match_type = clean_text(match.get('type', ''))[:25]
            print(f"       {i+1}. {match.get('date', 'No date')} | {organizer}... | {match_type}...")
    
    # Check for duplicates
    print(f"\nüîç DUPLICATE DETECTION:")
    duplicates_found = find_duplicates(tier2_data)
    print(f"   Potential duplicates: {len(duplicates_found)}")
    
    if duplicates_found:
        for i, dup in enumerate(duplicates_found[:3]):
            print(f"   {i+1}. {dup['date']}: {dup['match1']['organizer'][:30]}... vs {dup['match2']['organizer'][:30]}...")
    
    # Compare with previous results
    print(f"\nüìà IMPROVEMENT COMPARISON:")
    print(f"   BEFORE (original Tier 2):")
    print(f"     Total matches: 142")
    print(f"     Suspicious matches: 24")
    print(f"     Potential duplicates: 69")
    print(f"     Empty organizers: ~20%")
    print(f"   AFTER (updated scraper):")
    print(f"     Total matches: {len(tier2_data)}")
    print(f"     Suspicious matches: {total_issues}")
    print(f"     Potential duplicates: {len(duplicates_found)}")
    print(f"     Empty organizers: {sum(1 for m in tier2_data if not clean_text(m.get('organizer', '')))/len(tier2_data)*100:.1f}%")
    
    # Final assessment
    print(f"\nüéØ FINAL ASSESSMENT:")
    if total_issues < 5 and len(duplicates_found) < 5:
        print("‚úÖ TIER 2 FIXES: SUCCESSFUL!")
        print("‚úÖ Field mappings are working correctly")
        print("‚úÖ Data quality significantly improved")
        print("‚úÖ Deduplication is effective")
        print("‚úÖ Recommendation: Ready for production use")
    elif total_issues < 15 and len(duplicates_found) < 15:
        print("‚ö†Ô∏è  TIER 2 FIXES: GOOD PROGRESS")
        print(f"‚ö†Ô∏è  Minor issues remain: {total_issues} suspicious, {len(duplicates_found)} duplicates")
        print("‚úÖ Significant improvement over previous version")
    else:
        print("‚ùå TIER 2 FIXES: STILL NEED WORK")
        print(f"‚ùå Issues found: {total_issues} suspicious, {len(duplicates_found)} duplicates")
    
    # Show some suspicious matches if any
    if all_suspicious_matches:
        print(f"\nüö® SUSPICIOUS MATCHES (first 3):")
        for i, suspicious in enumerate(all_suspicious_matches[:3]):
            match = suspicious['match']
            issues = suspicious['issues']
            print(f"   {i+1}. {suspicious['calendar_type']} - {match.get('date', 'No date')}:")
            print(f"      Organizer: '{clean_text(match.get('organizer', ''))}'")
            print(f"      Type: '{clean_text(match.get('type', ''))}'")
            print(f"      Issues: {', '.join(issues)}")

def clean_text(text):
    """Clean text by removing escape characters and normalizing whitespace"""
    if not text:
        return ""
    
    # Remove escape characters and email artifacts
    text = text.replace('\\', '')
    text = re.sub(r'\[email.*?protected\]', '', text)
    text = re.sub(r'u00[a-fA-F0-9]{2}', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def find_duplicates(matches):
    """Find potential duplicate matches"""
    duplicates = []
    
    # Group by date
    by_date = defaultdict(list)
    for match in matches:
        by_date[match.get('date', '')].append(match)
    
    for date, date_matches in by_date.items():
        if len(date_matches) > 1:
            # Check for duplicates within the same date
            for i, match1 in enumerate(date_matches):
                for j, match2 in enumerate(date_matches[i+1:], i+1):
                    org1 = clean_text(match1.get('organizer', ''))
                    org2 = clean_text(match2.get('organizer', ''))
                    loc1 = clean_text(match1.get('location', ''))
                    loc2 = clean_text(match2.get('location', ''))
                    
                    # Check for high similarity
                    from difflib import SequenceMatcher
                    org_sim = SequenceMatcher(None, org1.lower(), org2.lower()).ratio()
                    loc_sim = SequenceMatcher(None, loc1.lower(), loc2.lower()).ratio()
                    
                    if org_sim > 0.8 or loc_sim > 0.8:
                        duplicates.append({
                            'date': date,
                            'match1': match1,
                            'match2': match2,
                            'org_similarity': org_sim,
                            'loc_similarity': loc_sim
                        })
    
    return duplicates

if __name__ == "__main__":
    validate_updated_scraper_results() 